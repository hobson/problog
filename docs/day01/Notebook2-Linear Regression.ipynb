{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib notebook \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numdifftools as nd\n",
    "from sklearn import datasets, linear_model\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35f22f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Loading and Processing Data\n",
    "\n",
    "We will use california housing data from the sklearn library's dataset for the demo. This dataset has 20,640 samples with 9 feature values. We will use only a subset of this dataset and regress Median Income over Median House Value to be able to visualize the fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV and columns\n",
    "df = datasets.fetch_california_housing(as_frame=True)['frame']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of the predictor and target variables\n",
    "Y = df['MedHouseVal'].to_numpy()[:1000]\n",
    "X = df['MedInc'].to_numpy()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the two variables\n",
    "%matplotlib\n",
    "plt.scatter(X,Y)\n",
    "plt.title('Data')\n",
    "plt.xlabel('Median Income')\n",
    "plt.ylabel('Median House Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15150275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the data for training and evaluation\n",
    "X=X.reshape(len(X),1)\n",
    "Y=Y.reshape(len(Y),1)\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "X_train = X[:-200]\n",
    "X_test = X[-200:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "Y_train = Y[:-200]\n",
    "Y_test = Y[-200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a1f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Linear Regression Analytical Solution\n",
    "\n",
    "We use sklearn's implementation of the Analytical solution of the coefficients of regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model using training set\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "# Plot outputs\n",
    "%matplotlib\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.title('Data')\n",
    "plt.xlabel('Median Income')\n",
    "plt.ylabel('Median House Value')\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.plot(X_train, regr.predict(X_train), color='red',linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b20aa8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Linear Regression with GD\n",
    "\n",
    "Next, we calculate the same regression fit using different variations of the Gradient Descent algorithm. <br>\n",
    "In the follwing function LR_through_GD, we can set the batch size to 1 for Stochastic Gradient Descent Algorithm, to 800 (size of the train dataset) for Batch Gradient Descent Algorithm and any value in between for Mini-Batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f39dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing MiniBatch Gradient Descent \n",
    "def LR_through_GD(init, lr, epochs, X_train, Y_train, batch_size = 50):\n",
    "    '''\n",
    "    Perform linear regression using Mini-Batch Gradient Descent.\n",
    "    \n",
    "    Parameters:\n",
    "        init (list): Initial values for slope and intercept [w, c].\n",
    "        lr (float): Learning rate for gradient descent.\n",
    "        epochs (int): Number of optimization steps.\n",
    "        X_train (numpy.ndarray): Training input data.\n",
    "        Y_train (numpy.ndarray): Training target data.\n",
    "        batch_size (int): Size of each mini-batch.\n",
    "        \n",
    "    Returns:\n",
    "        history (list): List of [w, c] values at each step.\n",
    "    '''\n",
    "    history = []\n",
    "    w = init[0]\n",
    "    c = init[1]\n",
    "    n = float(len(X_train)) # number of training data\n",
    "            \n",
    "    num_batches = int(n / batch_size)  # Calculate the number of batches\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for batch_idx in range(num_batches):\n",
    "            \n",
    "            # Fetch data accordingly\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            batch_X = X_train[start_idx:end_idx]\n",
    "            batch_Y = Y_train[start_idx:end_idx]\n",
    "            \n",
    "            batch_size_actual = len(batch_X)\n",
    "            if(batch_size_actual <= 0):\n",
    "                continue;\n",
    "            \n",
    "            \n",
    "            Y_pred = w * batch_X + c   # The current predicted values of Y for the batch\n",
    "            \n",
    "            ###### TODO: \n",
    "            \n",
    "            ### calcuate the derivative wrt to w, c\n",
    "\n",
    "            D_w =  TODO  # Derivative wrt w\n",
    "            D_c =  TODO  # Derivative wrt c\n",
    "            \n",
    "            ##### Finish TODO\n",
    "            \n",
    "            w = w - learning_rate_slider.value * D_w  # Update w\n",
    "            c = c - learning_rate_slider.value * D_c  # Update c\n",
    "            \n",
    "            history.append([m, c])\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1070a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_LR(history):\n",
    "    '''\n",
    "    Visualize the linear regression optimization process.\n",
    "    \n",
    "    Parameters:\n",
    "        history (list): List of [w, c] values at each step.\n",
    "        \n",
    "    Returns:\n",
    "        anim (FuncAnimation): Animation of the optimization process.\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.set_xlabel('Income')\n",
    "    ax.set_ylabel('Price')\n",
    "    # ax.grid()\n",
    "\n",
    "    scatter, = ax.plot(X_train, Y_train,'o')\n",
    "    line, = ax.plot([], [], 'r')\n",
    "\n",
    "    def update(i):\n",
    "        y_pred = history[i][0]*X_train + history[i][1]\n",
    "        line.set_data(X_train, y_pred)\n",
    "\n",
    "    anim = FuncAnimation(fig, update, frames=len(history), interval=100, repeat = False)\n",
    "    plt.show()\n",
    "    return anim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Interactive Widgets\n",
    "global_animation = None\n",
    "\n",
    "# Create a button widget\n",
    "start_button = widgets.Button(description=\"Start Visualization\", layout=widgets.Layout(width='auto'))\n",
    "\n",
    "# Create an Output widget\n",
    "output = widgets.Output()  \n",
    "\n",
    "# Create sliders for learning rate and initial point\n",
    "learning_rate_slider =  widgets.FloatSlider(value=0.01, min=0.01, max=.15, step=0.01, description=\"Learning Rate:\")\n",
    "init_slider =  widgets.FloatSlider(value=1.0, min=-5.0, max=5.0, step=0.1, description=\"Initial Point:\")\n",
    "epochs_slider =  widgets.IntSlider(value=500, min=50, max=1000, step=50, description=\"Iterations:\")\n",
    "batch_slider =  widgets.IntSlider(value=50, min=1, max=800, step=1, description=\"Batch Size:\")\n",
    "\n",
    "\n",
    "def start_visualization(_):\n",
    "    with output: \n",
    "        clear_output(wait=True)\n",
    "        global global_animation\n",
    "        learning_rate = learning_rate_slider.value\n",
    "        init = init_slider.value\n",
    "        history = LR_through_GD([init, init], learning_rate, epochs_slider.value, X_train, Y_train, batch_slider.value)\n",
    "        plt.close()\n",
    "        global_animation= visualize_LR(history)\n",
    "        \n",
    "start_button.on_click(start_visualization)\n",
    "\n",
    "\n",
    "# Arrange the widgets using HBox and VBox\n",
    "widget_box = widgets.VBox([learning_rate_slider, init_slider, epochs_slider, batch_slider, start_button])\n",
    "print(\"You can adjust the initial point and batch size using interactive widgets to see how they affect the optimization.\" +\n",
    "     \"\\nBatch Size 1 corresponds to SGD and batch size 800 is equivalent to batch gradient descent algorithm\")\n",
    "display(widget_box, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc8c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_sgd = LR_through_GD([0.0,0.0], 0.01, 500, X_train, Y_train, 1)\n",
    "history_mini_batch = LR_through_GD([0.0,0.0], 0.01, 500, X_train, Y_train, 200)\n",
    "history_batch = LR_through_GD([0.0,0.0], 0.01, 500, X_train, Y_train, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb26f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analytical Solution : c= \", regr.intercept_, \" w= \", regr.coef_ )\n",
    "print(\"Solution from SGD : c= \", history_sgd[-1][1], \" w= \", history_sgd[-1][0])\n",
    "print(\"Solution from Mini Batch GD : c= \", history_mini_batch[-1][1], \" w= \", history_mini_batch[-1][0])\n",
    "print(\"Solution from Batch GD : c= \", history_batch[-1][1], \" w= \", history_batch[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca7d46",
   "metadata": {},
   "source": [
    "#### What version of SGD has the closest estimates to the Analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b64fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
