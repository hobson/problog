# CoT reasoning project (problog webapp) 

### Meeting notes

See notes: [problog/docs/chain-of-thought-reasoning-large-numbers.md](https://github.com/hobson/problog/blob/main/docs/chain-of-thought-reasoning-large-numbers.md)

### References

#### longer math word problems
https://arxiv.org/pdf/2405.14804
Repository missing: https://github.com/XinXU-USTC/CoLeG-Math 
- extended math word problem dataset used llms to extend context length
- favorable results seem to rely on cheating (data leakage)

#### cumulative reasoning
 https://arxiv.org/pdf/2308.04371

https://github.com/XinXU-USTC/cumulative-reasoning

https://github.com/XinXU-USTC/
- cumulative reasoning instead of cot claims 43% improvement on hard problems but these are selected Wikipedia FOL problems that cite their source, so problems llms trained on

- hs math problem dataset includes geometry problems and dot file format diagrams in answers

#### TinyGSM 

https://arxiv.org/pdf/2312.09241v1
12.3M math problem training set from gsm8k with Python solutions generated by gpt3.5 
Solutions verified (scored and then best selected) to get 81.5% accuracy (better than gpt 3.5) with 2.7B params for gsm8k (but overfit on it)
2 code examples in paper would be parametrizable and we should imitate their docstring problem statement format but as fstrings or jinja2 templates

#### Example parameterized GSM8k problem
```yaml
-
  N_00: 48
  question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
  question_template: Natalia sold clips to {N_00} of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
  N_01_formula: 48/2
  N_01: 24
  answer: Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.
  answer_template: Natalia sold {N_00}/2 = {N_01} clips in May. Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.
  answer_numerical: 72
```

#### Datasets

- TinyGSM 
  - https://huggingface.co/datasets/TinyGSM/TinyGSM
  - can pair with gsm8k equivalents to parameterize python code
  - cross contamination only check 13 grams after removing punctuation,
    - should replace numbers with {number} before n-gram check
    - Should replace names with {name}
    - Should replace all tokens with POS 
    - replace named entities with named entity class
    - should resolve coreferences with SpaCy
    - calculate diversity of logic patterns
    - add python equation solving sym package code to solve
- GSM8k
  - Math word problem datasets GSM8K (Cobbe et al., 2021) 
  - https://paperswithcode.com/dataset/gsm8k
  - https://huggingface.co/datasets/openai/gsm8k
  - body+question answer+explanation+equation_annotation
  - answer contains math formula which can be evaluated in python
  - named entities (including pronouns) can be extracted and parameterized
  - nouns for objects can be parameterized as lists of possible alternatives
  - dereferencing of pronouns (coreference resolution) might be used to obfuscate the problem making it harder for machines
  - numbers in question can be paired with numbers in answer
- MAWPS (Koncel-Kedziorski et al.,
2016)
  - https://aclanthology.org/N16-1136.pdf
  - https://paperswithcode.com/dataset/mawps
  - https://github.com/sroy9/mawps (java)
  - https://huggingface.co/datasets/mwpt5/MAWPS/viewer
  - parameterized_question equation (N_00+N_01) answer (float) numbers (space delim floats)
- ASDiv (Miao et al., 2020)
  - https://huggingface.co/datasets/MU-NLPC/Calc-asdiv_a/viewer
  - data loader  https://huggingface.co/datasets/EleutherAI/asdiv/blob/main/asdiv.py
  - body+question equation answer grade-level
  - number words in question
  - grades 1-5
  - equation in HTML format chain of operations for calculator
- SVAMP (Patel et al., 2021) 
  - https://arxiv.org/pdf/2103.07191
  - https://huggingface.co/datasets/ChilleD/SVAMP/viewer
  - body, question, equation, answer (int)

#### Robustness Challenges
- irrelevant context - Shi et al. (2023a)
- new problems dissimilar to training set in wording (but not in math or logic)
- common sense knowledge
- numerical precision (robustness to large numbers)
- arithmetic complexity - number of steps for human to do arithmetic (1 digit vs 2 vs 3, powers of 10 vs 5 vs 2 vs 12 vs 6 vs 1 (prime numbers), addition vs multiplication, multiplication vs long division)
- https://huggingface.co/datasets/TinyGSM/TinyGSM
- 
#### Approaches
- verifier
- fine tuning
- tool use (calculator)